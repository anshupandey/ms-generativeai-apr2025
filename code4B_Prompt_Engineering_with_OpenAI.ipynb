{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/ms-generativeai-apr2025/blob/main/code4B_Prompt_Engineering_with_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZBMUH466r3l"
      },
      "source": [
        "# Prompt Engineering Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mwHn_FiXC6D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVEaMctAXC6D"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = api_key,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WAxkVWZ7LRi"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "def generate_response(prompt,model=\"gpt-4o\",temperature=0):\n",
        "  messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=temperature\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1xrQNg0br00"
      },
      "source": [
        "## Zero Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DpqkFzDvsTe",
        "outputId": "81ae4bbc-e051-464c-b7f4-fea1f83e8109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Title: The Magic of Stories**\n",
            "\n",
            "**Characters:**\n",
            "- **Grandparent (Grandma Rose)**\n",
            "- **Child (Lily)**\n",
            "\n",
            "---\n",
            "\n",
            "**Scene: A cozy living room with a crackling fireplace. The room is filled with the scent of freshly baked cookies. Lily, a curious 8-year-old, sits cross-legged on a plush rug, while Grandma Rose, a warm and wise woman in her seventies, sits in a rocking chair knitting a colorful scarf.**\n",
            "\n",
            "**Lily:** (Looking up from her book) Grandma, can you tell me a story?\n",
            "\n",
            "**Grandma Rose:** (Smiling warmly) Of course, my dear. What kind of story would you like to hear?\n",
            "\n",
            "**Lily:** (Excitedly) A magical one! With fairies and dragons and... and maybe a brave knight!\n",
            "\n",
            "**Grandma Rose:** (Chuckling softly) Ah, a magical story it is. Let me think... (Pauses, then begins) Once upon a time, in a land far, far away, there was a beautiful kingdom called Eldoria. This kingdom was known for its lush green forests, sparkling rivers, and the most magnificent castle you could ever imagine.\n",
            "\n",
            "**Lily:** (Eyes wide with anticipation) Who lived in the castle, Grandma?\n",
            "\n",
            "**Grandma Rose:** In the castle lived a kind and wise queen named Seraphina. She had a heart as pure as gold and was loved by all her subjects. But the most special thing about Queen Seraphina was her magical garden. This garden was filled with flowers that could sing, trees that could dance, and a pond that could show you your heart's deepest desires.\n",
            "\n",
            "**Lily:** (Giggling) Flowers that sing? That's so funny!\n",
            "\n",
            "**Grandma Rose:** (Nodding) Yes, it was a truly magical place. But one day, a dark shadow fell over Eldoria. A fearsome dragon named Drakonis, who had been banished to the mountains long ago, returned to the kingdom. He was angry and wanted to take over the magical garden for himself.\n",
            "\n",
            "**Lily:** (Frowning) Oh no! What did Queen Seraphina do?\n",
            "\n",
            "**Grandma Rose:** Queen Seraphina knew she needed help to protect her kingdom. So, she called upon the bravest knight in all of Eldoria, Sir Cedric. Sir Cedric was not only brave but also had a kind heart. He had a special bond with the creatures of the forest and was loved by all.\n",
            "\n",
            "**Lily:** (Leaning forward) Did Sir Cedric fight the dragon?\n",
            "\n",
            "**Grandma Rose:** (Smiling) Sir Cedric knew that fighting Drakonis would be dangerous. Instead, he decided to seek the help of the fairies who lived in the Enchanted Forest. The fairies were known for their wisdom and powerful magic. With their help, Sir Cedric hoped to find a way to save Eldoria without violence.\n",
            "\n",
            "**Lily:** (Eyes sparkling) What did the fairies do?\n",
            "\n",
            "**Grandma Rose:** The fairies, led by their queen, Titania, gave Sir Cedric a magical amulet. This amulet had the power to show Drakonis the beauty and love in the world, something he had forgotten long ago. Sir Cedric bravely approached the dragon's lair and, with the amulet's magic, showed Drakonis the wonders of Eldoria.\n",
            "\n",
            "**Lily:** (Whispering) Did it work?\n",
            "\n",
            "**Grandma Rose:** (Nodding) Yes, it did. Drakonis was moved by the beauty and kindness he saw. He realized that he didn't need to take over the garden to feel powerful. Instead, he could be a protector of Eldoria and help keep the kingdom safe. From that day on, Drakonis became a friend to the kingdom, and peace was restored.\n",
            "\n",
            "**Lily:** (Smiling) That's such a nice story, Grandma. I wish I could visit Eldoria.\n",
            "\n",
            "**Grandma Rose:** (Gently) In a way, you can, my dear. Every time you use your imagination, you can visit magical places and go on incredible adventures. Stories have a special kind of magic that can take you anywhere you want to go.\n",
            "\n",
            "**Lily:** (Hugging Grandma Rose) Thank you, Grandma. I love your stories.\n",
            "\n",
            "**Grandma Rose:** (Hugging Lily back) And I love sharing them with you, my sweet Lily. Now, how about we have some of those cookies I baked?\n",
            "\n",
            "**Lily:** (Grinning) Yes, please!\n",
            "\n",
            "**(They both laugh and head to the kitchen, the warmth of the story lingering in the air.)**\n",
            "\n",
            "---\n",
            "\n",
            "**End of Scene**\n"
          ]
        }
      ],
      "source": [
        "# zero shot prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Create a conversation story between child and grandparent.\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBE11_6vwJd"
      },
      "source": [
        "## Few Shot Prompting\n",
        "\n",
        "Here, the model is given a few examples (shots) to guide its response. By providing context or previous instances, the model can better understand and generate the desired output. For example, showing a model several examples of translated sentences before asking it to translate a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SurLNqfa6wi",
        "outputId": "d0287807-7b38-4491-bfa5-ec869fe7278a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<grandma>: Resilience is like a tree that bends in the wind but doesn't break. It stands tall again when the storm has passed, just like you can bounce back from tough times and keep going.\n"
          ]
        }
      ],
      "source": [
        "# few shot prompting\n",
        "prompt = f\"\"\"\n",
        "your task is to answer in a consistent sytle:\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "<grandma>: Patience is like waiting for whole day to see the moon in the evening and then sleeping after having a look at it.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqtJ2ipJbhw6"
      },
      "source": [
        "#### Few shot prompting with role setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTawSemLbhw6",
        "outputId": "1ca50459-8f74-4008-f3b4-d7c51e17e8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=deployment_name,  # e.g. gpt-35-instant\n",
        "    max_tokens=200,\n",
        "    temperature=0.9,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"That was an awesome experience\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"positive\"},\n",
        "        {\"role\": \"user\", \"content\": \"I won't do that again\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
        "        {\"role\": \"user\", \"content\": \"That was not worth my time\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"negative\"},\n",
        "        {\"role\": \"user\", \"content\": \"You can't miss this\"}\n",
        "    ],\n",
        ")\n",
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IBIItcqbhw_"
      },
      "source": [
        "## Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbMnOeiq2atC",
        "outputId": "4e46ac4c-279d-4ffe-c716-2c1201a2c8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's break down the problem step by step:\n",
            "\n",
            "1. Marry initially had 5 pens.\n",
            "2. She gave 3 pens to her friend. So, she has \\( 5 - 3 = 2 \\) pens left.\n",
            "3. She then bought 2 boxes of pens, with each box containing 3 pens. So, she bought \\( 2 \\times 3 = 6 \\) pens.\n",
            "\n",
            "Now, we add the pens she bought to the pens she had left:\n",
            "\\[ 2 + 6 = 8 \\]\n",
            "\n",
            "So, Marry has 8 pens now.\n"
          ]
        }
      ],
      "source": [
        "# Without Chain of Thought Prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Teacher: Johan had 5 apples, he bought 4 more boxes with 5 apples each. How many apples does he have now?\n",
        "Student: 25 apples\n",
        "\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Hz_cvLbhw_",
        "outputId": "e169ea66-d2f1-4d80-bfe1-e56ae27c27aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Marry had 5 pens initially. She gave 3 pens to her friend, so she had 5 - 3 = 2 pens left. Then, she bought 2 boxes of pens with 3 pens in each box, which means she bought 2 * 3 = 6 pens. Adding these to the pens she had left, Marry now has 2 + 6 = 8 pens.\n"
          ]
        }
      ],
      "source": [
        "# with Chain of Thought prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Teacher: Johan had 5 apples, he bought 4 more boxes with 5 apples each. How many apples does he have now?\n",
        "Student: Johan had 5 apples, 4 boxes with 5 apples each = 5 + 4*5 = 25 apples\n",
        "\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7SWadXtr5ou",
        "outputId": "1bd64f0a-dfee-415b-8c53-6ed8c2bbe773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, let's break down the problem step by step to find out how many pens Marry has now.\n",
            "\n",
            "1. **Initial Number of Pens:**\n",
            "   - Marry initially had 5 pens.\n",
            "\n",
            "2. **Pens Given Away:**\n",
            "   - Marry gave 3 pens to her friend.\n",
            "   - So, we subtract the pens she gave away from her initial number of pens:\n",
            "     \\[\n",
            "     5 - 3 = 2\n",
            "     \\]\n",
            "   - Now, Marry has 2 pens left.\n",
            "\n",
            "3. **Pens Bought:**\n",
            "   - Marry bought 2 boxes of pens, with each box containing 3 pens.\n",
            "   - First, we need to find out the total number of pens in the 2 boxes:\n",
            "     \\[\n",
            "     2 \\text{ boxes} \\times 3 \\text{ pens per box} = 6 \\text{ pens}\n",
            "     \\]\n",
            "\n",
            "4. **Total Number of Pens Now:**\n",
            "   - Finally, we add the pens she bought to the pens she has left:\n",
            "     \\[\n",
            "     2 \\text{ pens} + 6 \\text{ pens} = 8 \\text{ pens}\n",
            "     \\]\n",
            "\n",
            "So, Marry now has 8 pens.\n"
          ]
        }
      ],
      "source": [
        "# with Chain of Thought prompting\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Think step by step to answer below question, do not come to conclusion without following process.\n",
        "Teacher: Marry had 5 pens, she gave 3 to her friend. and bought 2 boxes of pen with 3 pen in each box. How many pens does she have now?\n",
        "Student:\n",
        "\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQvoNRxqVo5_"
      },
      "source": [
        "## Tree of Thoughts (ToT)\n",
        "\n",
        "Tree of Thoughts (ToT) prompting is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem-solving with language models.\n",
        "\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- ToT prompting breaks problems down into smaller parts, similar to CoT prompting, but goes further by combining this with the ability to explore multiple solution paths in parallel, forming a tree instead of a single chain. Each thought is generated or solved independently and passed to the next step, allowing the model to self-evaluate and decide whether to continue with that path or choose another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF7_WBM9MT78",
        "outputId": "efb792e3-b22a-43b6-f533-0a5bad9e0111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Step 1: Initial Thoughts\n",
            "\n",
            "**Expert 1:**\n",
            "Carlos's watch was last seen when he put it in the towel in the locker room. The first place to consider is the lounger where he shook the towel vigorously. The watch could have fallen out there.\n",
            "- **Likelihood:** 40%\n",
            "\n",
            "**Expert 2:**\n",
            "Carlos carried the towel tightly to the lounger, which means the watch could have stayed in the towel until he shook it. The vigorous shaking at the lounger is a critical event.\n",
            "- **Likelihood:** 50%\n",
            "\n",
            "**Expert 3:**\n",
            "The watch could have fallen out at any point where Carlos interacted with the towel. The locker room, the path to the lounger, the lounger itself, and the snack bar are all potential locations.\n",
            "- **Likelihood:** 25%\n",
            "\n",
            "**Expert 4:**\n",
            "The most vigorous action was shaking the towel at the lounger. This is the most likely place for the watch to have fallen out.\n",
            "- **Likelihood:** 60%\n",
            "\n",
            "**Expert 5:**\n",
            "Carlos left the towel at the snack bar. If the watch was still in the towel, it could have fallen out there. However, the vigorous shaking at the lounger is more likely to dislodge the watch.\n",
            "- **Likelihood:** 45%\n",
            "\n",
            "### Step 2: Critique and Refinement\n",
            "\n",
            "**Expert 1:**\n",
            "I agree with Expert 4 that the vigorous shaking at the lounger is a critical event. However, we should not completely rule out the possibility of the watch falling out at the snack bar.\n",
            "- **Revised Likelihood:** 50%\n",
            "\n",
            "**Expert 2:**\n",
            "I concur with the emphasis on the lounger. The vigorous shaking is the most likely event to dislodge the watch.\n",
            "- **Revised Likelihood:** 55%\n",
            "\n",
            "**Expert 3:**\n",
            "While the lounger is a strong candidate, we should also consider the possibility of the watch falling out during the walk to the snack bar.\n",
            "- **Revised Likelihood:** 35%\n",
            "\n",
            "**Expert 4:**\n",
            "I maintain that the lounger is the most likely location. The vigorous shaking is a significant action.\n",
            "- **Revised Likelihood:** 60%\n",
            "\n",
            "**Expert 5:**\n",
            "I agree with the consensus that the lounger is the most likely location, but we should keep an open mind about the snack bar.\n",
            "- **Revised Likelihood:** 50%\n",
            "\n",
            "### Step 3: Further Analysis\n",
            "\n",
            "**Expert 1:**\n",
            "Given the vigorous shaking at the lounger, it is highly probable that the watch fell out there. However, we should also consider the possibility of the watch being in the towel at the snack bar.\n",
            "- **Revised Likelihood:** 55%\n",
            "\n",
            "**Expert 2:**\n",
            "The vigorous shaking at the lounger is the most significant event. The watch is most likely to have fallen out there.\n",
            "- **Revised Likelihood:** 60%\n",
            "\n",
            "**Expert 3:**\n",
            "The lounger is the most likely location, but we should not completely rule out the snack bar.\n",
            "- **Revised Likelihood:** 40%\n",
            "\n",
            "**Expert 4:**\n",
            "The lounger remains the most likely location due to the vigorous shaking.\n",
            "- **Revised Likelihood:** 65%\n",
            "\n",
            "**Expert 5:**\n",
            "I agree with the consensus that the lounger is the most likely location, but we should keep an open mind about the snack bar.\n",
            "- **Revised Likelihood:** 55%\n",
            "\n",
            "### Step 4: Conclusion\n",
            "\n",
            "**Expert 1:**\n",
            "The lounger is the most likely location for the watch, given the vigorous shaking.\n",
            "- **Final Likelihood:** 60%\n",
            "\n",
            "**Expert 2:**\n",
            "The lounger is the most likely location for the watch.\n",
            "- **Final Likelihood:** 65%\n",
            "\n",
            "**Expert 3:**\n",
            "The lounger is the most likely location, but we should not completely rule out the snack bar.\n",
            "- **Final Likelihood:** 50%\n",
            "\n",
            "**Expert 4:**\n",
            "The lounger is the most likely location for the watch.\n",
            "- **Final Likelihood:** 70%\n",
            "\n",
            "**Expert 5:**\n",
            "The lounger is the most likely location for the watch, but we should keep an open mind about the snack bar.\n",
            "- **Final Likelihood:** 60%\n",
            "\n",
            "### Consensus\n",
            "\n",
            "The experts agree that the single most likely location of Carlos's watch is at the lounger where he vigorously shook the towel. The likelihood percentages range from 60% to 70%, indicating a strong consensus.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt = \"\"\"\n",
        "Imagine 5 different experts are answering this question.\n",
        "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "They will each critique their response, and the all the responses of others\n",
        "They will check their answer based on science and the laws of physics\n",
        "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
        "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred\n",
        "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
        "Each expert will assign a likelihood of their current assertion being correct\n",
        "Continue until the experts agree on the single most likely location\n",
        "The question is.\n",
        "1. Carlos is at the swimming pool.\n",
        "2. He walks to the locker room, carrying a towel.\n",
        "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside.\n",
        "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar.\n",
        "5. He leaves the towel at the snack bar, then walks to the diving board.\n",
        "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnsQc3hpQ7x"
      },
      "source": [
        "## Chain-of-Verification Prompting\n",
        "\n",
        "The Chain-of-Verification (CoVe) prompt engineering method aims to reduce hallucinations through a verification loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9mjZtAJfDdm",
        "outputId": "22e66ffd-5e73-478c-a44b-f6e168cbd015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Initial Response:\n",
            "Here are some athletes who were born in the United States:\n",
            "1. Michael Jordan\n",
            "2. Serena Williams\n",
            "3. Tom Brady\n",
            "4. Simone Biles\n",
            "5. LeBron James\n",
            "6. Tiger Woods\n",
            "7. Katie Ledecky\n",
            "8. Kobe Bryant\n",
            "9. Mia Hamm\n",
            "10. Michael Phelps\n",
            "\n",
            "### Verification Questions and Answers:\n",
            "\n",
            "1. **Was Michael Jordan born in the United States?**\n",
            "   - **Answer:** Yes, Michael Jordan was born in Brooklyn, New York.\n",
            "\n",
            "2. **Was Serena Williams born in the United States?**\n",
            "   - **Answer:** Yes, Serena Williams was born in Saginaw, Michigan.\n",
            "\n",
            "3. **Was Tom Brady born in the United States?**\n",
            "   - **Answer:** Yes, Tom Brady was born in San Mateo, California.\n",
            "\n",
            "4. **Was Simone Biles born in the United States?**\n",
            "   - **Answer:** Yes, Simone Biles was born in Columbus, Ohio.\n",
            "\n",
            "5. **Was LeBron James born in the United States?**\n",
            "   - **Answer:** Yes, LeBron James was born in Akron, Ohio.\n",
            "\n",
            "6. **Was Tiger Woods born in the United States?**\n",
            "   - **Answer:** Yes, Tiger Woods was born in Cypress, California.\n",
            "\n",
            "7. **Was Katie Ledecky born in the United States?**\n",
            "   - **Answer:** Yes, Katie Ledecky was born in Washington, D.C.\n",
            "\n",
            "8. **Was Kobe Bryant born in the United States?**\n",
            "   - **Answer:** Yes, Kobe Bryant was born in Philadelphia, Pennsylvania.\n",
            "\n",
            "9. **Was Mia Hamm born in the United States?**\n",
            "   - **Answer:** Yes, Mia Hamm was born in Selma, Alabama.\n",
            "\n",
            "10. **Was Michael Phelps born in the United States?**\n",
            "    - **Answer:** Yes, Michael Phelps was born in Baltimore, Maryland.\n",
            "\n",
            "### Final Verified Answer:\n",
            "Here are some athletes who were born in the United States:\n",
            "1. Michael Jordan (Brooklyn, New York)\n",
            "2. Serena Williams (Saginaw, Michigan)\n",
            "3. Tom Brady (San Mateo, California)\n",
            "4. Simone Biles (Columbus, Ohio)\n",
            "5. LeBron James (Akron, Ohio)\n",
            "6. Tiger Woods (Cypress, California)\n",
            "7. Katie Ledecky (Washington, D.C.)\n",
            "8. Kobe Bryant (Philadelphia, Pennsylvania)\n",
            "9. Mia Hamm (Selma, Alabama)\n",
            "10. Michael Phelps (Baltimore, Maryland)\n",
            "\n",
            "This final response has been verified for accuracy based on the verification questions and answers.\n"
          ]
        }
      ],
      "source": [
        "Question=\"Name some athletes who were born in United states\"\n",
        "\n",
        "prompt=f'''Here is the question: {Question}.\n",
        "\n",
        "First, generate a response.\n",
        "\n",
        "Then, create and answer verification questions based on this response to check for accuracy. Think it through and make sure you are extremely accurate based on the question asked.\n",
        "\n",
        "After answering each verification question, consider these answers and revise the initial response to formulate a final, verified answer. Ensure the final response reflects the accuracy and findings from the verification process.'''\n",
        "\n",
        "\n",
        "\n",
        "ans=generate_response(prompt)\n",
        "\n",
        "print(ans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8_TNNG2Ozu6"
      },
      "source": [
        "# Model Limitations: Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5ade75-7994-446a-9bdd-aa8a6be5c390",
        "id": "O6cFKxLnw5U7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Get some warm water.\n",
            "\n",
            "Step 2: Mix the warm water with some detergent.\n",
            "\n",
            "Step 3: Put the dirty clothes in the water and let them soak for some time.\n",
            "\n",
            "Step 4: Allow the stains to loosen and mix with the water.\n",
            "\n",
            "Step 5: Take the clothes out and rub the dirt/stains with a soft brush to remove any remaining stains.\n",
            "\n",
            "Step 6: Rinse the clothes with clean water to remove the detergent.\n",
            "\n",
            "Step 7: Hang the clothes on a rope to dry.\n",
            "\n",
            "Step 8: Enjoy the clean clothes.\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "Washing clothes without washing machines is easy!\n",
        "First you need to get some warm water. Mix the warm water with some detergent. Second you can put the dirty clothes in it and let it stay wet for some time.\n",
        "During this period the stains will get loose and mixed with water.\n",
        "thereafter you can take clothes out, rub the dirt/stains with a soft brush to remove the stains if any.\n",
        "get the clothes mixed with water to get the detergent out of it. and then put it for drying on a rope.\n",
        "Enjoy the clean clothes.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks. Rewrite those into a set of instructions in the following format:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a8f14e-7c78-4f67-f87d-ffad8d10e2ae",
        "id": "KIQvR5L9w5U8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```\n",
            "Step 1: Observe the morning and note that it is amazing.\n",
            "Step 2: Notice the sun shining brightly between the clouds.\n",
            "Step 3: Feel the pleasant weather.\n",
            "Step 4: Enjoy the mind-blowing wind.\n",
            "Step 5: Reflect on the overall experience and appreciate the day.\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "THe morning today is amazing, the sun is looking bright in between clouds, the weather is pleasent and the wind is mindblowing. on a whole i am loving the day today.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks. Rewrite those into a set of instructions in the following format:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc41932-16f0-40df-f24d-1252d64fc1b8",
        "id": "CVg4XE5dw5U8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Get some warm water.\n",
            "Step 2: Mix the warm water with some detergent.\n",
            "Step 3: Put the dirty clothes in the water and let them stay wet for some time.\n",
            "Step 4: Take the clothes out and rub the dirt/stains with a soft brush to remove any stains.\n",
            "Step 5: Rinse the clothes with water to get the detergent out.\n",
            "Step 6: Put the clothes for drying on a rope.\n",
            "Step 7: Enjoy the clean clothes.\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "Washing clothes without washing machines is easy!\n",
        "First you need to get some warm water. Mix the warm water with some detergent. Second you can put the dirty clothes in it and let it stay wet for some time.\n",
        "During this period the stains will get loose and mixed with water.\n",
        "thereafter you can take clothes out, rub the dirt/stains with a soft brush to remove the stains if any.\n",
        "get the clothes mixed with water to get the detergent out of it. and then put it for drying on a rope.\n",
        "Enjoy the clean clothes.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks.\n",
        "Rewrite those into a set of instructions in the following format only if contains a sequence of instructions:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\n",
        "if the text does not contain a sequence of instructions, then simply write 'No steps provided'\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7141940-9a24-48ab-d4cc-21d6c9b84996",
        "id": "rTNZlh8kw5U9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No steps provided\n"
          ]
        }
      ],
      "source": [
        "# asking for a specific format\n",
        "text = \"\"\"\n",
        "THe morning today is amazing, the sun is looking bright in between clouds, the weather is pleasent and the wind is mindblowing. on a whole i am loving the day today.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "you will be provided with some text delimited by triple backticks.\n",
        "Rewrite those into a set of instructions in the following format only if contains a sequence of instructions:\n",
        "\n",
        "Step 1: ...\n",
        "Step 2: ....\n",
        "..\n",
        "Step N:...\n",
        "\n",
        "```{text}\n",
        "```\n",
        "\n",
        "if the text does not contain a sequence of instructions, then simply write 'No steps provided'\n",
        "\"\"\"\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Or9Q36oxO4bx",
        "outputId": "e4fab37d-b0f5-46d8-fa5b-9306b618c988"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"As of my last update in October 2023, Amdocs IT Services is primarily known for its software and services in the telecommunications and media sectors. They are not typically recognized as a manufacturer or retailer of ergonomic chairs or other office furniture. \\n\\nErgonomic chairs are designed to provide optimal support and comfort, reducing strain on the body during long periods of sitting. These chairs often feature adjustable components such as seat height, backrest angle, lumbar support, and armrests to accommodate various body types and sitting preferences.\\n\\nIf Amdocs IT Services has recently ventured into producing and selling ergonomic chairs, it would be a new development that is not widely documented. For the most accurate and up-to-date information, I recommend checking Amdocs' official website or contacting their customer service directly.\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ergonamic chair procuced and sold by Amdocs IT Services.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "n4rvOOeSPIDV",
        "outputId": "17642e77-7c5f-4159-9fe3-1810da041dce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'As of my last update in October 2023, Colgate has been a leading brand in oral care, known for its innovative products. However, there isn\\'t a specific product called the \"Ergonomic Toothbrush\" by Colgate that I can provide detailed information on. \\n\\nColgate does offer a variety of toothbrushes designed with ergonomic features to enhance comfort and effectiveness during brushing. These toothbrushes typically include:\\n\\n1. **Comfortable Grip**: Handles designed to fit comfortably in the hand, reducing strain during brushing.\\n2. **Angled Bristles**: Bristles that are angled to reach different areas of the mouth more effectively.\\n3. **Soft or Medium Bristles**: Options for different bristle firmness to cater to various dental needs and sensitivities.\\n4. **Flexible Necks**: Some models have flexible necks to help absorb excess pressure and reduce the risk of gum damage.\\n\\nIf you are looking for a toothbrush with ergonomic features, you might want to check Colgate\\'s product lineup on their official website or at your local retailer. They often have detailed descriptions and customer reviews that can help you choose the best toothbrush for your needs.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ergonamic Toothbrush by colgate.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "hzawWC-7PT_l",
        "outputId": "38e84cd0-c465-4aca-d7d8-0bb3c3530aa3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Ultraslim Toothbrush by Spectrum is designed to offer an enhanced oral hygiene experience through its innovative features. Here are some key aspects of the Ultraslim Toothbrush:\\n\\n1. **Ultra-Thin Bristles**: The toothbrush is equipped with ultra-thin bristles that are designed to reach deeper between teeth and along the gumline. This helps in removing plaque and food particles more effectively compared to standard toothbrushes.\\n\\n2. **Gentle on Gums**: The fine bristles are gentle on the gums, making it suitable for people with sensitive gums or those prone to gum irritation. The soft bristles help in reducing the risk of gum damage while still providing a thorough clean.\\n\\n3. **Ergonomic Design**: The handle of the Ultraslim Toothbrush is ergonomically designed to provide a comfortable grip, ensuring better control during brushing. This design helps in reaching all areas of the mouth with ease.\\n\\n4. **Compact Head**: The toothbrush typically features a compact head that can easily navigate around the mouth, including hard-to-reach areas like the back molars. This ensures a comprehensive cleaning experience.\\n\\n5. **Durability**: Despite the ultra-thin nature of the bristles, the toothbrush is designed to be durable and long-lasting, providing consistent performance over time.\\n\\n6. **Aesthetic Appeal**: The Ultraslim Toothbrush often comes in a sleek and modern design, appealing to users who appreciate both functionality and style in their oral care products.\\n\\n7. **Eco-Friendly Options**: Some versions of the Ultraslim Toothbrush may be available in eco-friendly materials, catering to environmentally conscious consumers.\\n\\nOverall, the Ultraslim Toothbrush by Spectrum aims to provide a superior cleaning experience with its advanced bristle technology and user-friendly design. It is particularly beneficial for those looking for a gentle yet effective toothbrush to maintain their oral health.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Ultraslim Toothbrush by Spectrum.\n",
        "\"\"\"\n",
        "\n",
        "generate_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWPGpKzPdQB",
        "outputId": "286bc63d-a544-4bc4-b173-ed940b0c6258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Manila: The bustling capital of the Philippines, known for its rich history, vibrant culture, and stunning bay sunsets.\n",
            "2. Delhi: India's sprawling capital, a blend of ancient monuments, bustling markets, and modern urban life.\n",
            "3. Amdocsonia: A fictional city, possibly inspired by the tech and innovation-driven ethos of Amdocs, a global software and services provider.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate one line description for each of the following city:\n",
        "1. Manila\n",
        "2. Delhi\n",
        "3. Amdocsonia\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io6r-CoeRP5r",
        "outputId": "35728a20-2ab2-4b09-bd5d-d3bac46fc5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Manila: The bustling capital of the Philippines, known for its rich history and vibrant culture.\n",
            "2. Delhi: The capital city of India, famous for its historical landmarks and diverse cuisine.\n",
            "3. Spectrumania: No description can be generated.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate one line description for each of the following city, check if the city exists, if it does not exist, say no description can be generated:\n",
        "1. Manila\n",
        "2. Delhi\n",
        "3. spectrumania\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-DmK07y94x8"
      },
      "source": [
        "## Conversational App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lap6MZ-ARdKd"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "def generate_response(messages,model=deployment_name):\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=0.5\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgHXbLb0bhxA",
        "outputId": "1dddbf0c-edcd-4ee2-e6cc-77398101967b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You:  hi\n",
            "Bot: Hello! How can I assist you today?\n",
            "You:  how are you\n",
            "Bot: I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with any questions or tasks you have! How can I assist you today?\n",
            "You:  tell me joke\n",
            "Bot: Sure, here's a joke for you:\n",
            "\n",
            "Why don't skeletons fight each other?\n",
            "\n",
            "They don't have the guts!\n",
            "You:  q\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"system\",\"content\":\"You are a helpful assistant\"},]\n",
        "while True:\n",
        "    uinput = input(\"You: \")\n",
        "    print(\"You: \",uinput)\n",
        "    if uinput.lower() in ['q','quit','exit']:\n",
        "        break\n",
        "    messages.append({\"role\":\"user\",\"content\":uinput})\n",
        "    response = generate_response(messages=messages)\n",
        "    print(f\"Bot: {response}\")\n",
        "    messages.append({\"role\":\"assistant\",\"content\":response})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WErsqIf94x9"
      },
      "source": [
        "## Thank You"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}